# These parameters are dependant on the sensor location
# Note: For converting angles use https://www.andre-gaschler.com/rotationconverter/
perception_node:
  ros__parameters:

    # Transformation from sensor to world frame
    lidar2world_transform:
      # [x,y,z] in meters
      translation: [0.0, 0.0, 3.0]

      # [w,x,y,z] normalized quaternion
      quaternion: [0.999769, 0.0, -0.021498, 0.0]

    # The crop box isolates a region of interest in the pointcloud
    # all point outside this box are removed
    # Use the roi_visulaizer.py tool to find a new crop_box params if needed
    crop_box_transform:
      # [x,y,z] in meters
      translation: [35.0, -2.0, 1.75]

      # [w,x,y,z] normalized quaternion
      quaternion: [0.898794046299167, 0.0, 0.0, 0.4383711467890774]

      # [x, y, z] in meters
      size: [18.5, 75.0,3.5]


# Use the same `lidar2world_transform` for both nodes
projection_node:
  ros__parameters:

    # Matrix from intrinsic camera calibration
    # This day's data was recorded at 1920x1080
    camera_matrix: [1199.821557, 0.0, 960.562236, 0.0, 1198.033465, 551.675808, 0.0, 0.0, 1.0]

    # Transformation from sensor to world frame
    lidar2world_transform:
      # [x,y,z] in meters
      translation: [0.0, 0.0, 3.0]

      # [w,x,y,z] normalized quaternion
      quaternion: [0.999769, 0.0, -0.021498, 0.0]

    # Transform from Lidar to camera frame
    lidar2cam_extrinsic:
      rotation: [0.0, 1.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0]
      translation: [0.00, 0.0508, 0.0502]

    # Transform from lidar data from to lidar sensor frame
    # See https://github.com/ros-drivers/ros2_ouster_drivers/issues/87
    # The translation part may already be accounted for internally in the driver
    lidarData2lidarSensor_extrinsic:
      rotation: [-1.0, 0.0, 0.0, 0.0, -1.0, 0.0, 0.0, 0.0, 1.0]
      translation: [0.0, 0.0, 0.03618]

